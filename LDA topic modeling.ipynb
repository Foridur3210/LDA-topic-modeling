{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Anaconda3.7\\lib\\site-packages\\past\\types\\oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Iterable\n",
      "C:\\Users\\admin\\Anaconda3.7\\lib\\site-packages\\past\\builtins\\misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "from spacy import load\n",
    "\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spacy is loaded for text processing tasks.\n",
    "nlp = load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of data set is 19955.\n"
     ]
    }
   ],
   "source": [
    "# Load Corpus\n",
    "corpus= pd.read_csv('C:/Users/admin/Desktop/topic modelling/20newsgroup.csv', encoding='utf-8')\n",
    "data_set = corpus['Text']\n",
    "print(\"Size of data set is %s.\" %str(len(data_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _nlp_layer(x_series):\n",
    "    print(\"Performing text normalization.\")\n",
    "    contractions_list = {\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"i'll've\": \"i will have\",\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i'am\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it had\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"that'd\": \"that would\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there had\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we had\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'alls\": \"you alls\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you had\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you you will\",\n",
    "        \"you'll've\": \"you you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    c_re = re.compile('(%s)' % '|'.join(contractions_list.keys()))\n",
    "\n",
    "    def expand_contractions(text, c_re_=c_re):\n",
    "        def replace(match):\n",
    "            return contractions_list[match.group(0)]\n",
    "\n",
    "        return c_re_.sub(replace, text)\n",
    "\n",
    "    x = [re.sub('[^0-9a-z\\' ]+', ' ', item.lower()).split() for item in x_series]\n",
    "    processed_x = []\n",
    "    for eachQuery in x:\n",
    "        query = []\n",
    "        for eachToken in eachQuery:\n",
    "            expanded_token = expand_contractions(eachToken)\n",
    "            # Applied to normalize words like word's, mother's, 1980's etc.\n",
    "            if '\\'' in expanded_token:\n",
    "                expanded_token = expanded_token.strip('\\'s')\n",
    "            # Replace alpha-numeric with special token 'Special_Tok'\n",
    "            if expanded_token.isalnum() and not expanded_token.isalpha() and not expanded_token.isdigit():\n",
    "                expanded_token = 'SpecialTok'\n",
    "            # Replace all numerals with special token - 'NUM'\n",
    "            if expanded_token.isdigit():\n",
    "                expanded_token = 'NUM'\n",
    "            query.extend(expanded_token.split())\n",
    "        processed_x.append(query)\n",
    "    return processed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing text normalization.\n"
     ]
    }
   ],
   "source": [
    "data_set_processed = _nlp_layer(data_set)\n",
    "corpus_for_tf = []\n",
    "for each_document in data_set_processed:\n",
    "    each_file = \"\"\n",
    "    each_file += \" \".join(each_token for each_token in each_document)\n",
    "    corpus_for_tf.append(each_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(ngram_range=(1,1),\n",
    "                                max_features=100000,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(corpus_for_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA model with term frequency features, n_samples=19955 and n_features=100000.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "                          evaluate_every=-1, learning_decay=0.7,\n",
       "                          learning_method='online', learning_offset=50.0,\n",
       "                          max_doc_update_iter=100, max_iter=50,\n",
       "                          mean_change_tol=0.001, n_components=20, n_jobs=None,\n",
       "                          perp_tol=0.1, random_state=0, topic_word_prior=None,\n",
       "                          total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Fitting LDA model with term frequency features, n_samples=%d and n_features=%d.\" % (tf.shape[0], tf.shape[1]))\n",
    "lda = LatentDirichletAllocation(n_components=20, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \",\".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model and associated terms:\n",
      "Topic #0: specialtok,ax,num,max,pl,db,cx,bhj,giz,wm,ah,sl,bh,mq,gk,chz,bxn,mv,hz,lk \n",
      "\n",
      "Topic #1: armenian,turkish,armenians,men,said,children,people,women,arab,muslims,muslim,armenia,sex,sexual,history,cancer,islam,today,party,years \n",
      "\n",
      "Topic #2: virginia,henry,myers,harvard,spencer,acc,hess,ryan,liver,erik,vote,murdoch,motto,waldrop,zoology,dzkriz,uio,galaxy,ndet,bob \n",
      "\n",
      "Topic #3: edu,atf,mil,umd,navy,ra,cs,ai,uga,michael,colostate,boeing,wam,maine,acns,racism,rpi,john,div,oakland \n",
      "\n",
      "Topic #4: specialtok,com,edu,writes,article,uiuc,netcom,cso,sin,andrew,news,cmu,hell,brian,mary,david,isc,opinions,ca,steve \n",
      "\n",
      "Topic #5: nra,tires,motorcycles,circuits,arf,tire,lehigh,seats,lunar,gretzky,nsmca,obo,kilometers,logo,tampa,moncton,transportation,adirondack,champs,kirlian \n",
      "\n",
      "Topic #6: num,specialtok,new,israel,christ,edu,university,april,guns,national,american,period,bike,st,war,team,ed,john,total,league \n",
      "\n",
      "Topic #7: people,god,government,say,think,writes,believe,does,don,article,law,right,jesus,know,way,fact,just,edu,evidence,did \n",
      "\n",
      "Topic #8: specialtok,drive,num,card,dos,windows,disk,hard,monitor,drives,drivers,driver,board,os,controller,problem,ide,cards,floppy,does \n",
      "\n",
      "Topic #9: insurance,rutgers,edu,amendment,purdue,gatech,printer,hp,georgia,print,rsa,prism,canon,flyers,ecn,murray,cc,laser,lemieux,nl \n",
      "\n",
      "Topic #10: cdt,jets,espn,clinical,packet,gld,rec,selanne,je,regulated,autos,canucks,muscle,forsale,formation,royalroads,launchers,slip,restaurant,tigers \n",
      "\n",
      "Topic #11: stephanopoulos,rider,ripem,ufl,wolverine,comics,hulk,lunar,plo,biker,sabretooth,deadly,liefeld,issue,riders,mariner,psych,keith,cryptosystem,chromium \n",
      "\n",
      "Topic #12: specialtok,edu,writes,just,like,don,article,know,think,com,time,good,ve,going,people,really,way,make,did,want \n",
      "\n",
      "Topic #13: adl,duke,istanbul,bullock,printf,bony,ankara,foods,den,livni,nutrition,traded,ermeni,arens,define,yigal,placebo,sinus,osmanli,pad \n",
      "\n",
      "Topic #14: batf,hiv,honda,shell,cup,elohim,portal,gerard,blah,geb,lds,pluto,mcconkie,cadre,scorer,lyme,hernlem,dsl,latin,blood \n",
      "\n",
      "Topic #15: nhl,azerbaijan,leafs,bruins,karabakh,azeri,islanders,stl,nyr,armenia,azeris,goalie,gilmour,fuhr,lindros,baku,tb,bds,jagr,twin \n",
      "\n",
      "Topic #16: cramer,optilink,drugs,japanese,gaza,sternlight,palestine,dyer,zionism,partners,ucsc,thor,steveh,hamburg,strnlght,deserved,cpr,bontchev,japan,hendricks \n",
      "\n",
      "Topic #17: num,specialtok,use,edu,file,program,mail,data,software,available,information,com,key,thanks,using,space,computer,bit,image,files \n",
      "\n",
      "Topic #18: apple,kent,sandvik,temperature,se,cpu,newton,sweden,mouse,candida,mhz,yeast,gear,cd,centris,henrik,brake,mercury,detectors,nubus \n",
      "\n",
      "Topic #19: pitcher,pitching,hitter,sox,batting,col,kaldis,mets,line,ok,int,contest,jays,play,pitchers,rules,char,hr,eof,oname \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Topics in LDA model and associated terms:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
